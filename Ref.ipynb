{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Information Retrieval\n",
    "## Instructions\n",
    "1. Students will form teams of three people each and submit a single homework for each team in the format - ID1_ID2_ID3.ipynb\n",
    "2. Groups of four are not allowed and you are more than welcome to form groups of two.\n",
    "2. **Do not write your names anywhere.**\n",
    "3. For the code part: \n",
    "> **Write your code only in the mentioned sections. Do not change the code of other sections**. Do not use any imports unless we say so.\n",
    "4. For theoretical questions, if any - write your answer in the markdown cell dedicated to this task, in **English**.\n",
    "\n",
    "\n",
    "#### Deviation from the aforementioned  instructions will lead to reduced grade\n",
    "---\n",
    "\n",
    "\n",
    "## Clarifications\n",
    "1. The same score for the homework will be given to each member of the team.  \n",
    "2. The goal of this homework is to test your understanding of the concepts presented in the lectures. \\\n",
    "If a topic was not covered in detail during the lecture, you are asked to study it online on your own. \n",
    "Anyhow, we provide here detailed explanations for the code part and if you have problems - ask.\n",
    "3. Questions can be sent to the forum, you are encouraged to ask questions but do so after you have been thinking about your question. \n",
    "4. The length of the empty gaps (where you are supposed to write your code) is a recommendation (the amount of space took us to write the solution) and writing longer code will not harm your grade. We do not expect you to use the programming tricks and hacks we used to make the code shorter.   \n",
    "Having said that, we do encourage you to write good code and keep that in mind - **extreme** cases may be downgraded.  \n",
    "We also encourage to use informative variable names - it is easier for us to check and for you to understand. \n",
    "\n",
    "For your convenience, , the code has a **DEBUG** mode that you may use in order to debug with toy data.  \n",
    "It is recommended to solve the code in that mode (with efficiency in mind) and then run the code on all the data.\n",
    "**Do not forget to file the HW with DEBUG == False**.\n",
    "\n",
    "`Download the data` from [HERE](https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics) and put it in the same directory your script is.\n",
    "\n",
    "Since it is the first time we provide this homework please notify us if there is a bug/something is unclear, typo's exc..\n",
    "\n",
    "5. We use Python 3.7 for programming.\n",
    "6. Make sure you have all the packages and functions used in the import section. Most of it is native to Anaconda Python distribution.\n",
    "\n",
    "### Have fun !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from typing import List,Dict\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\elino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\elino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "from string import punctuation, ascii_lowercase\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug\n",
    "\"\"\" you can change this cell \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\"\"\"\n",
    "Recommended to start with a small number to get a feeling for the preprocessing with prints (N_ROWS_FOR_DEBUG = 2)\n",
    "later increase this number for 5*10**3 in order to see that the code runs at reasonable speed. \n",
    "When setting Debug == False, our code implements bow.fit() in 15-20 minutes according to the tqdm progress bar. Your solution is not supposed to be much further than that.\n",
    "\"\"\"\n",
    "N_ROWS_FOR_DEBUG = 5*10**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = Path(\"lyrics.csv\")\n",
    "BOW_PATH = Path(\"bow.csv\")\n",
    "N_ROWS = N_ROWS_FOR_DEBUG if DEBUG else None\n",
    "CHUNCK_SIZE = 5 if DEBUG else 5*10**3\n",
    "tqdm_n_iterations = N_ROWS//CHUNCK_SIZE +1 if DEBUG else 363*10**3//CHUNCK_SIZE + 1\n",
    "COLS = [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bag of words model\n",
    "### Implement the following methods:\n",
    "\n",
    "* `preprocess_sentence`: \n",
    "    * Lower case the word\n",
    "    * Ignores it if it's in the stopwords list\n",
    "    * Removes characters which are not in the allowed symbols\n",
    "    * Stems it and appends it to the output sentence\n",
    "    * Discards words with length <= 1\n",
    "    \n",
    "    \n",
    "* `update_counts_and_probabilities`: \n",
    "\n",
    "    * Update self.unigram count (the amount of time each word is in the text)\n",
    "    * Update self.bigram count (two consecutive word occurances)\n",
    "    * Update self.trigram count (three consecutive word occurances)\n",
    "    * Update inverted index: a dictionary with words as keys and the values is a dictionary - {'DocID' : word_count}   \n",
    "    \n",
    "* `compute_word_document_frequency`:\n",
    "\n",
    "   * For each word count the number of docs it appears in. For example , for the word 'apple' -\n",
    "$$\\sum_{i \\in docs} I(apple \\in doc_i), I := Indicator function$$\n",
    "\n",
    "\n",
    "* `update_inverted_index_with_tf_idf_and_compute_document_norm`:\n",
    "\n",
    "    * Update the inverted index (which currently hold word counts) with tf idf weighing. We will compute tf by dividing with the number of words in each document. \n",
    "    * As we want to calculate the document norm, incrementally update the document norm. pay attention that later we apply sqrt to it to finish the process.\n",
    "\n",
    "#### The result of this code is a bag of words model that already counts for TF-IDF weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "allowed_symbols = set(l for l in ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 73/73 [18:28<00:00, 15.19s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This function get an input sentnce and returns new sentnce according to the preprocess rules that where defined\"\"\"\n",
    "def preprocess_sentence(sentence : str) -> List[str]:\n",
    "    output_sentence = []\n",
    "    for word in word_tokenize(sentence):                           \n",
    "        lower_word=word.lower()            #Lower case the word\n",
    "        if lower_word not in stop_words:   #Ignores it if it's in the stopwords list\n",
    "            letters=list(lower_word)\n",
    "            good_letters=[]\n",
    "            for l in letters:\n",
    "                if l in allowed_symbols:\n",
    "                    good_letters.append(l)  #Removes characters which are not in the allowed symbols\n",
    "            new_word=''.join(good_letters)  #Reconnect the good letters back into one word\n",
    "            new_word=stemmer.stem(new_word) #stream the word\n",
    "            if len(new_word)>1:             #Discards words with length <= 1\n",
    "                output_sentence.append(new_word)\n",
    "    return output_sentence\n",
    "\n",
    "\n",
    "def get_data_chuncks() -> List[str]:                           \n",
    "    for i ,chunck in enumerate(pd.read_csv(INPUT_FILE_PATH, usecols = COLS, chunksize = CHUNCK_SIZE, nrows = N_ROWS)):\n",
    "        chunck = chunck.values.tolist()\n",
    "        yield [chunck[i][0] for i in range(len(chunck))] \n",
    "\n",
    "class TfIdf:\n",
    "    def __init__(self):\n",
    "        self.unigram_count =  Counter()\n",
    "        self.bigram_count = Counter()\n",
    "        self.trigram_count = Counter()\n",
    "        self.document_term_frequency = Counter()\n",
    "        self.word_document_frequency = {}\n",
    "        self.inverted_index = {}\n",
    "        self.doc_norms = {}\n",
    "        self.n_docs = -1\n",
    "        self.sentence_preprocesser = preprocess_sentence\n",
    "        self.bow_path = BOW_PATH\n",
    "\n",
    "    \"\"\"This function get an input sentnce and update the unigram, bigram and trigram counter objects. moreover\n",
    "    the function update the inverted index dict with the number of occurrence for each word in the doc\"\"\"  \n",
    "    def update_counts_and_probabilities(self, sentence :List[str],document_id:int) -> None:\n",
    "        sentence_len = len(sentence)\n",
    "        self.document_term_frequency[document_id] = sentence_len\n",
    "        for i,word in enumerate(sentence):\n",
    "            self.unigram_count[word]+=1     #update the unigram Counter object that holds for each word how many time it appers in the corpus\n",
    "            if i<(sentence_len-1):          #edge condition to check that we are not in the last word\n",
    "                y=word,sentence[i+1]        #take the word and the adjacent word in the sentence\n",
    "                self.bigram_count[y]+=1     #update the bigram Counter object that holds for each pair of words how many time it appers in the corpus\n",
    "            if i<(sentence_len-2):          #edge condition to check that we are not in word befor the last word\n",
    "                z=word,sentence[i+1],sentence[i+2] #take the word and the two adjacent words in the sentence\n",
    "                self.trigram_count[z]+=1     #update the trigram Counter object that holds for each trio of words how many time it appers in the corpus\n",
    "            if word not in self.inverted_index:\n",
    "                word_dict={}                 #initialize new empty dict that will be the value in the inverted_index dict for the key 'word'\n",
    "                self.inverted_index[word]=word_dict #update the inverted_index dict with the new value 'word_dict'\n",
    "            word_dict1={}                    #initialize new empty dict and insert to it the doc id as key and the unigram_count of the word as value\n",
    "            word_dict1[document_id]=self.unigram_count[word] #insert to the empy dict the doc id as key and the unigram_count of the word as value\n",
    "            current_dict=self.inverted_index[word] #get the current value from inverted_index dict for the key 'word'. this value is a dict with doc id and counts  \n",
    "            update=current_dict.update(word_dict1) #add to the current dict 'word_dict1' -the new key- value pair that calculated in that loop iteration\n",
    "            self.inverted_index[word]=current_dict #replace the old value in the inverted_index dict with the new updated dict (with the current doc and word count) \n",
    "             \n",
    "    def fit(self) -> None:\n",
    "        for chunck in tqdm(get_data_chuncks(), total = tqdm_n_iterations):         \n",
    "            for sentence in chunck:                                                 \n",
    "                self.n_docs += 1\n",
    "                if not isinstance(sentence, str):\n",
    "                    continue\n",
    "                sentence = self.sentence_preprocesser(sentence)\n",
    "                if sentence:\n",
    "                    self.update_counts_and_probabilities(sentence,self.n_docs)\n",
    "        self.save_bow() # bow is 'bag of words'\n",
    "        self.compute_word_document_frequency()                                      \n",
    "        self.update_inverted_index_with_tf_idf_and_compute_document_norm()\n",
    "        \n",
    "    \"\"\"This function count for each word the number of docs it appears in\"\"\"                                                                                 \n",
    "    def compute_word_document_frequency(self):\n",
    "        for word in self.inverted_index.keys():     #for each uniqe word, update the word document frequency as the number of docs that the word appers in\n",
    "            self.word_document_frequency[word]=len(self.inverted_index[word])\n",
    "            \n",
    "    \"\"\"\"This function run over all the words in the inverted index dict and for each word and doc replace the word count\n",
    "    with the tf-idf weighting between the word and the doc. Moreover, the function calculate the docs norm\"\"\"        \n",
    "    def update_inverted_index_with_tf_idf_and_compute_document_norm(self):\n",
    "        for word in self.inverted_index.keys():           #for each word in the inverted_index keys\n",
    "            for doc in self.inverted_index[word].keys():  #run over all the the docs that the word apper in\n",
    "                len_doc=self.document_term_frequency[doc] #get the doc len\n",
    "                dict_word=self.inverted_index[word]       #get the dict that holds for the word the docs that it appear in it and how many times \n",
    "                term_frequency=(dict_word[doc])/len_doc   #calculated the term frequency as the number of times the word appears in the doc divide to the number of words in the doc \n",
    "                w_ij=term_frequency*(np.log10((self.n_docs+1)/self.word_document_frequency[word])) #calcluated tf idf weighting: term frequency double in idf(log 10 of the number of docs, divded in the number of docs that the word appers in )               \n",
    "                first_dict=self.inverted_index[word]       #get the current value from the inverted index dict-a dict with doc as key and counts as values.\n",
    "                first_dict[doc]=w_ij                       #change the value for the key 'doc' in the word dict from the word count in the dict to the calculated weight   \n",
    "                if doc not in self.doc_norms:\n",
    "                     self.doc_norms[doc]=0                 #initialize value for the first time that creating the doc norm\n",
    "                self.doc_norms[doc]+=(w_ij**2)             #in order to calculated the norm, we will sum up the square weights and in the end take the sqrt \n",
    "\n",
    "        for doc in self.doc_norms.keys():\n",
    "            self.doc_norms[doc] = np.sqrt(self.doc_norms[doc]) \n",
    "            \n",
    "    def save_bow(self):\n",
    "        pd.DataFrame([self.inverted_index]).T.to_csv(self.bow_path)\n",
    "                \n",
    "tf_idf = TfIdf()\n",
    "tf_idf.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Bag of words model:\n",
    "\n",
    "1. What is the computational complexity of this model, as a function of the number of docs in the corpus?\n",
    "2. How can we make this code better in terms running time (parallelization or other topics you find)? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. \n",
    "Definition:\n",
    "D-The number of docs in the corpus\n",
    "V- The number of all the words in the corpus =200 (We assumed that 200 is the average number of words in one song)\n",
    "We will ignore the number of the characters in each word, because the average number of the word's length is 5 (not so big).\n",
    "\n",
    "we start with the fit function:\n",
    "    -The function goes over all the documents- O(D)\n",
    "        *Inside, calling to preprocess_sentence function and it goes over the words in the documents- O(V)\n",
    "        *Then, calling to update_counts_and_probabilities function and it goes over the good words in a document- in the worst          case it's all the original words- O(V)\n",
    "\t^^Here we got O(D)*(O(V)+O(V))=O(D)*O(V)=O(D*V)\n",
    "\n",
    "\t-Now, calling to compute_word_document_frequency function, and it goes over the words in the inverted index- in the worst        case it's all the original words- O(V)\n",
    "\t^^Here we got O(V)\n",
    "\n",
    "    -Finally, calling to update_inverted_index_with_tf_idf_and_compute_document_norm function and it goes over the words in the      inverted index- in the worst case it's all the original words- O(V)\n",
    "        *Inside go all over the documents in the keys of the values in the inverted index- O(D)\n",
    "    -Then, goes all over the documents in the keys of doc_norms dictionary- O(D)\n",
    "    ^^Here we got O(V)*O(D)+O(D)=O(D*V)+O(D)=O(D*V)\n",
    "    \n",
    "In conclusion we got: O(D*V)+O(V)+O(D*V)=O(D*V)+O(V)=O(D*V)\n",
    "If V=200 and D=5*(10^3) ----> O(200*5*(10^3))\n",
    "\n",
    "\n",
    "\n",
    "2.\n",
    "We can join 3 processes that currently occurring separately to one process that will make all 3 occur at the same time.\n",
    "The processes that occur in 'update inverted index' and 'compute word document frequency' functions can be entered into the 'update_counts_and_probabilities' function.\n",
    "For each word in the for loop in 'update_counts_and_probabilities' function we will update the inverted_index directly with weights instead of word counts so that it will be a dictionary with word as key and dictionary as value -  {'DocID' : weight=tf*idf} and for the first appearance of a certain word in the document we will add 1 to word_document_frequency[word]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DocumentRetriever\n",
    "Not this retriever &#8595;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![dsafdsafsdafdsf](https://cdn3-www.dogtime.com/assets/uploads/2019/10/golden-cocker-retriever-mixed-dog-breed-pictures-cover-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the following methods:\n",
    "\n",
    "`reduce_query_to_counts`: given a list of words returns a counter object with words as keys and counts as values.\n",
    "\n",
    "`rank`: given a query and relevant documents calculate the similarity (cosine or inner product simialrity) between each document and the query.   \n",
    "Make sure to transform the query word counts to tf idf as well. \n",
    "\n",
    "`sort_and_retrieve_k_best`: returns the top k documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    def __init__(self, tf_idf):\n",
    "        self.sentence_preprocesser = preprocess_sentence  \n",
    "        self.vocab = set(tf_idf.unigram_count.keys())\n",
    "        self.n_docs = tf_idf.n_docs\n",
    "        self.inverted_index = tf_idf.inverted_index\n",
    "        self.word_document_frequency = tf_idf.word_document_frequency\n",
    "        self.doc_norms = tf_idf.doc_norms\n",
    "\n",
    "    \"\"\"\"This function calculate the simmilarity between each doc and the query using inner product metric or cosine metric\"\"\"    \n",
    "    def rank(self,query : Dict[str,int],documents: Dict[str,Counter],metric: str ) -> Dict[str, float]:\n",
    "        result = {} # key: DocID , value : float , simmilarity to query\n",
    "        query_len = np.sum(np.array(list(query.values())))\n",
    "        query_norm = 0                           #initialize the query norm\n",
    "        query_w={}                               #initialize dict that holds the query words and their tf-idf weighting \n",
    "        for word in query:\n",
    "            tf=query[word]/query_len              #term frequency of the word is the number of times that the word appers in the query divided in the number of words in the query\n",
    "            query_w[word]=tf*(np.log10((self.n_docs+1)/self.word_document_frequency[word])) #calcluated tf idf weighting: term frequency double in idf(log 10 of the number of docs, divded in the number of docs that the word appers in )\n",
    "            query_norm+=(query_w[word])**2        #in order to calculated the norm, we will sum up the square weights and in the end take the sqrt \n",
    "            for doc in documents[word].keys():    #for each doc from the docs that the word appears in\n",
    "                if doc not in result:             #first time the doc appears in the final dict result\n",
    "                    result[doc]=0                 #initialize the doc value to zero\n",
    "                dic_word=documents[word]          #get the current value from the documents dict-a dict with doc id as key and weight between the word and the doc as value.\n",
    "                result[doc]+=(dic_word[doc]*query_w[word]) #sum up the inner_product calculation to the result dict (word-dict weighting duble the word-query weighting)      \n",
    "        if metric == 'cosine':                    #while using cosine and not inner_product, in the end need to divide the result in the query norm duble the doc norm\n",
    "            for key in result: result[key] = result[key]/(np.sqrt(query_norm)*self.doc_norms[key])   #doc_norms is already sqrt fron Q.1.1\n",
    "        return result\n",
    "        \n",
    "    \"\"\"\"This function get dict with the doc id and simmilarity and returns list of doc id's that in the top K docs with higher simmilarity\"\"\"\n",
    "    def sort_and_retrieve_k_best(self, scores: Dict[str, float],k :int): \n",
    "        return list((dict(Counter(scores).most_common(k))).keys()) \n",
    "\n",
    "    \"\"\"\"This function given a list of words returns a counter object with words as keys and counts as values\"\"\"\n",
    "    def reduce_query_to_counts(self, query : List)->  Counter:\n",
    "        return Counter(query)\n",
    "        \n",
    "        \n",
    "    def get_top_k_documents(self,query : str, metric: str , k = 5) -> List[str]:\n",
    "        query = self.sentence_preprocesser(query)\n",
    "        query = [word for word in query if word in self.vocab] # filter nan \n",
    "        query_bow = self.reduce_query_to_counts(query)\n",
    "        relavant_documents = {word : self.inverted_index.get(word) for word in query}\n",
    "        ducuments_with_similarity = self.rank(query_bow,relavant_documents, metric)\n",
    "        return self.sort_and_retrieve_k_best(ducuments_with_similarity,k)\n",
    "        \n",
    "dr = DocumentRetriever(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elino\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py:694: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "query = \"Better stop dreaming of the quiet life, 'cause it's the one we'll never know And quit running for that runaway bus 'cause those rosy days are few And stop apologizing for the things you've never done 'Cause time is short and life is cruel but it's up to us to change This town called malice\"\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[286904, 150547, 107350, 4673, 219069]\n",
      "[336918, 271639, 175238, 175220, 312772]\n"
     ]
    }
   ],
   "source": [
    "cosine_top_k = dr.get_top_k_documents(query, 'cosine')\n",
    "print(cosine_top_k)\n",
    "inner_product_top_k = dr.get_top_k_documents(query, 'inner_product')\n",
    "print(inner_product_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "song #0 \n",
      "I am thine inmost self\n",
      "I gaze out upon thy world of coloured lights from the darkness\n",
      "The darkness behind thine eyes\n",
      "I am the most ancient one, the creator of the gods\n",
      "I am the changing and the changless one\n",
      "I am the source of all that is\n",
      "In what thou called dreams, I gather my forces\n",
      "In what thou called reality, I stage my dreams\n",
      "I am the true god, the only god which is\n",
      "My dragon magic is sweet, for I will never grant thee thy wishes...\n",
      "I drink the life essence of those who exist only to serve my will\n",
      "Your dreams are my life, the bridge between the\n",
      "Transcendental metamorphosis take place\n",
      "As certain as the night follows the day\n",
      "Your mortal human avoidance of \"hurt feelings\" will cause you to feel -\n",
      "The pain of death\n",
      "Realize that the powers of the dreams are five, dimensions unseen...\n",
      "Astral reflections have immortalized me once, in a time before time\n",
      "Join those who've risen, bnU rEb kU cAn A, Hekal Tiamat!\n",
      "What follows is the breaking of the seventh seal\n",
      "The final harvest is close, see the banner of the winged skull of UR\n",
      "The gates of the greatest power shall be flung open -\n",
      "To the farthest reaches of all possible universe...\n",
      "Your world will be swept up with the agony of billions\n",
      "I am the undead who've risen, but time grows short for me\n",
      "I am patient 'cause I am eternal\n",
      "I am an elder god, the vampirelord - bow down before me \n",
      "##################################################\n",
      "##################################################\n",
      "song #1 \n",
      "I am the Guardian, the keeper of time\n",
      "I hold the seven keys, to places you never been\n",
      "I show you secrets, of things you never have seen\n",
      "So trust in your darkest thoughts,\n",
      "and follow your dream\n",
      "Dreams of another home,\n",
      "with pieces of tomorrow\n",
      "I've seen the future, and what is yet to be\n",
      "So raise your hands, and call to the gods\n",
      "Your destiny is ready, to lead the dreams\n",
      "I can behold your dreams, cause I gave you life\n",
      "Death is another home, of fallen angels\n",
      "Your life will become a thought, and you will forget\n",
      "So sleep into deeper minds, and dream to remember\n",
      "The dreams take me away, I'll become inhuman\n",
      "I touch the deeper soul, of eternity\n",
      "So raise your hands, and call to the gods\n",
      "Your destiny is ready, to lead the dreams \n",
      "##################################################\n",
      "##################################################\n",
      "song #2 \n",
      "I aint never been the settlin kind\n",
      "Then I met you and changed my mind\n",
      "I've always been the goin kind\n",
      "gone always gone all the time\n",
      "now Im missing all my rowdy friends\n",
      "Cause my single life is about to end\n",
      "Yes my single life is about to end\n",
      "I aint never been the leavin kind\n",
      "Ill be with you all the time\n",
      "I aint never been the stay home kind\n",
      "Now Im home always home all the time\n",
      "now Im missing all my rowdy friends\n",
      "Cause my single life is about to end\n",
      "Yes my single life is about to end \n",
      "##################################################\n",
      "##################################################\n",
      "song #3 \n",
      "The kings and queens send their pawns off to war\n",
      "An unfair law upheld in the years before\n",
      "They take a life from a child and their dreams\n",
      "Imagine people never even stop to think\n",
      "Back many years - Checkmate\n",
      "They believe to kill - Checkmate\n",
      "There was no law - Checkmate\n",
      "They killed at will - Checkmate\n",
      "Medieval torture - Checkmate\n",
      "Now begins - Checkmate\n",
      "Man to follow - Checkmate\n",
      "Many to hang - Checkmate\n",
      "Checkmate - Checkmate\n",
      "People die still, some at their youth\n",
      "No one knows why 'cause there never is no truth\n",
      "Romans believe to send their children off to war\n",
      "Retaliation, continuation of the war\n",
      "Retaliation of the war\n",
      "Some people die still, some still at their youth\n",
      "No one knows why, there is no truth\n",
      "Romans believe to send their children off to war\n",
      "Retaliation, continuation of the war \n",
      "##################################################\n",
      "##################################################\n",
      "song #4 \n",
      "Concentric circles\n",
      "Forever closing in\n",
      "Another travesty\n",
      "That never should have been\n",
      "The latest entry\n",
      "On an ever-growing list\n",
      "And yet you never change your ways\n",
      "Your denial still persists\n",
      "Run away, run away\n",
      "Run away from the setting sun\n",
      "Run away, run away\n",
      "Run away from the things you've done\n",
      "With accusing fingers\n",
      "You jut into the air\n",
      "To single out the ones\n",
      "You blame for your despair\n",
      "But the mirror's broken\n",
      "No reflection can it afford\n",
      "Only scattered glances\n",
      "Of it's pieces on the floor\n",
      "Run away, run away\n",
      "Run away from the setting sun\n",
      "Run away, run away\n",
      "Run away from the things you've done\n",
      "The ground beneath you\n",
      "Will crumble into dust\n",
      "The breath that forms your words\n",
      "Will never fill your lungs\n",
      "And as your senses dull\n",
      "A thought enters your mind\n",
      "\"I'm that one who caused all this\n",
      "There was no one else this time\"\n",
      "Run away, run away\n",
      "Run away from the setting sun\n",
      "Run away, run away\n",
      "Run away from the things you've done \n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "for index, song in enumerate(pd.read_csv(INPUT_FILE_PATH,usecols = [5]).iloc[cosine_top_k]['lyrics']):\n",
    "    sep = \"#\"*50\n",
    "    print(F\"{sep}\\nsong #{index} \\n{song} \\n{sep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 term statistics:\n",
    "Use \"TF-IDF\" object that we created earlier and answer the following questions:\n",
    "\n",
    "1. How many unique words we have?\n",
    "2. How many potential word bigrams we have? How many actual word bigrams we have? How do you explain this difference?\n",
    "3. What is the storage size of the input file \"lyrics.csv\"? What is the output file (bow.csv) size? how do you explain this difference?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words is:  388240\n",
      "Potenial word bigrams (worst case) is:  150730297600\n",
      "Actual word bigrams is:  6307797\n",
      "The storage size of bow.csv is:  249201756  Bytes\n",
      "The storage size of lyrcis.csv is:  324991390  Bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe lyrcis.csv has the full amount of words in the corpus, that includes words that appears more than once.\\nHowever, the bow.csv contain each word, that appears more than once in the corpus, only once (with the number of occurrences).\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. \n",
    "print ('Number of unique words is: ',len(tf_idf.inverted_index))\n",
    "\n",
    "\"\"\"\n",
    "When we initilize the inverted index we entered each word ones to the dictionary \n",
    "and just update the number of the occurance, Therfore all the words there are unique.\n",
    "In addition we could do that with the 'bow' object like that:\n",
    "data=pd.read_csv('bow.csv')\n",
    "df1=pd.DataFrame(data)\n",
    "print ('Number of unique words is: ',df1.shape[0])\n",
    "\"\"\"\n",
    "\n",
    "# 2.\n",
    "print ('Potenial word bigrams (worst case) is: ',((len(tf_idf.inverted_index))**2))\n",
    "print ('Actual word bigrams is: ',(len(tf_idf.bigram_count)))\n",
    "\n",
    "\"\"\"\n",
    "We'll compare the actual word biagrmas, which have only unique pairs combinations to the potential word biagram.\n",
    "Therfore, we want only the number of the unique pairs combinations. If we have V unique words in all of the corpus,\n",
    "we can say that the count of the potential word bigrams is V**2- This is the upper barrier.\n",
    "However, in the given data set, the number of pairs combinations is lower than this upper barrier, \n",
    "as there are repetitive word pairs combinations in the corpus.\n",
    "If we are talking not about the unique pairs cpombinations,so for one document we can get n-1 pairs combinations,\n",
    "when n is the number of all the words in the documents (not only the unique number of words).\n",
    "Therefore, for D documents we can get (n-1)*D pairs combinations (not unique).\n",
    "\"\"\"\n",
    "\n",
    "# 3.\n",
    "print ('The storage size of bow.csv is: ',BOW_PATH.stat().st_size, ' Bytes')\n",
    "print ('The storage size of lyrcis.csv is: ',INPUT_FILE_PATH.stat().st_size, ' Bytes')\n",
    "\n",
    "\"\"\"\n",
    "The lyrcis.csv has the full amount of words in the corpus, that includes words that appears more than once.\n",
    "However, the bow.csv contain each word, that appears more than once in the corpus, only once (with the number of occurrences).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 NgramSpellingCorrector\n",
    "Now we will implement a Ngarm (character Ngrams) spelling corrector. That is, we have an out of vocabulary word (w) and we want to retrieve the most similar words (in our vocabulary) to this word.\n",
    "we will model the similarity of two words by-\n",
    "\n",
    "$$sim(v,w) := prior \\cdot likelihood = p(w) \\cdot P(v|w) $$ \n",
    "$$P(v|w) := JaccardIndex =  \\frac{|X \\cap Y|}{|X \\cup Y|}$$\n",
    "\n",
    "Where v is an out of vocabulary word (typo or spelling mistake), w is in a vocabulary word, X is the ngram set of v and Y is the ngram set of w.\n",
    "For example, if n == 3, the set of ngrams for word \"banana\" is set(\"ban\",\"ana\",\"nan\",\"ana\") = {\"ban\",\"ana\",\"nan\"}\n",
    "\n",
    "In order to do it efficently, we will first construct an index from the possible Ngrams we have seen in our corpus to the words that those Ngrams appear in, in order prevent comparing w to all of the words in our corpus.\n",
    "Then, we will implement a function that computes this similarity.\n",
    "\n",
    "* Make sure you compute the JaccardIndex efficently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 2):\n",
    "        yield \"\".join(list(ngram))\n",
    "        \n",
    "def get_trigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 3):\n",
    "        yield \"\".join(list(ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramSpellingCorrector:\n",
    "    def __init__(self, unigram_counts: Counter, get_n_gram: callable):\n",
    "        self.unigram_counts = unigram_counts\n",
    "        self.ngram_index = {}\n",
    "        self.get_n_grams = get_n_gram\n",
    "        \n",
    "    '''This function creats the ngram_index. It goes all over the unique words that we have in the unigram dictionary,\n",
    "       and turn it to his ngrams. The keys in the dictionary are the ngrams, and the values are lists which contains\n",
    "       all the word which have this ngram'''    \n",
    "    def build_index(self) -> None:\n",
    "        for word in self.unigram_counts.keys():                  # Goes all over the word in the unigram\n",
    "            split_word=set(self.get_n_grams(word))               # Split the word by the get_n_gram which choosen, and turn it to set, which remove duplicate\n",
    "            for ngram in split_word:                             # Go all over the ngram we got from the splitting\n",
    "                if ngram not in self.ngram_index:                # If ngram not yed appended to the dictionary ngram_index\n",
    "                    self.ngram_index[ngram]=[]                   # Initilize the value of this ngram as empty list\n",
    "                self.ngram_index[ngram].append(word)             # For each ngram key, append to the value the word which certainly contains this ngrm\n",
    "        \n",
    "    '''This function takes the word-big string and split it by the ngram which choosen. For each ngram it goes over\n",
    "        the words which contains this n gram and split the word for calculating the sim between the word and the big\n",
    "        string. The function saving this sim and finally return the top K word with the biggest sim'''    \n",
    "    def get_top_k_words(self,word:str,k=5) -> List[str]:\n",
    "        dict_prob={}                                             # Initilize dictionary which contains the probabilty-sim for each word\n",
    "        split_str=set(self.get_n_grams(word))                    # Split the big string by the n_gram which, and turn it to set, which remove duplicate\n",
    "        sum_of_count=sum(self.unigram_counts.values())           # Save the number for the denominator in p(w)\n",
    "        for ngram in split_str:                                  # Goes over each ngram in the whole word\n",
    "            if ngram in self.ngram_index:                        # If ngram not in dictionary, it will continue\n",
    "                for word_n in self.ngram_index[ngram]:               # Goes over each word that contain this ngram\n",
    "                    if word_n not in dict_prob:\n",
    "                        split_word=set(self.get_n_grams(word_n))     # Split this word by the n_gram which, and turn it to set, which remove duplicate\n",
    "                        intersect=len(split_str&split_word)          # Got the number of ngram that are in both the word and the big string\n",
    "                        union=len(split_str|split_word)              # Got the number of ngram that are in the word and in the big string\n",
    "                        p_w=(self.unigram_counts[word_n])/(sum_of_count)       # Calculte the prior probability to get the word from all the words in the unigram\n",
    "                        sim=p_w*(intersect/union)                    # Calculte the sim for the big string and the word\n",
    "                        dict_prob[word_n]=sim                        # Append the sim to the dictionary under the word (the key)\n",
    "        return (list((dict(Counter(dict_prob).most_common(k))).keys()))                     # Return only the top K words, that for them the sim is biggest\n",
    "\n",
    "\n",
    "class BigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_bigrams)\n",
    "        \n",
    "        \n",
    "class TrigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_trigrams)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'caus', 'life', 'still', 'time']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_vocab_word = 'supercalifragilisticexpialidocious'\n",
    "bigram_spelling_corrector = BigramSpellingCorrector(tf_idf.unigram_count)\n",
    "bigram_spelling_corrector.build_index()\n",
    "bigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['life', 'still', 'call', 'listen', 'hous']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_spelling_corrector = TrigramSpellingCorrector(tf_idf.unigram_count)\n",
    "trigram_spelling_corrector.build_index()\n",
    "trigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Language model\n",
    "Calculate the log likelihood of a sentence. Once with a bigram markovian langauge model, and once with a trigram model.\n",
    "for example - the likelihood of the senetence \"spiderman spiderman does whatever a spider can\" for the bigram model is: \n",
    "$$p(spiderman)\\cdot p(spiderman|spiderman) \\cdot  (does|spiderman) \\cdot (whatever|does) \\cdot  (a|whatever) \\cdot  (spider|a) \\cdot (can|spider)$$\n",
    "\n",
    "And for the trigram model:\n",
    "$$p(spiderman,spiderman)\\cdot p(does|spiderman,spiderman) \\cdot  (whatever|spiderman,does) \\cdot (a|does,whatever) \\cdot  (spider|whatever,a) \\cdot  (can|a, spider)$$\n",
    "\n",
    "Since we do not want a zero probability sentence use Laplace smoothing, as you have seen in the lecture, or here https://en.wikipedia.org/wiki/Additive_smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram log likelihood is -80.95155390617043\n",
      "Trigram log likelihood is -75.24770210005998\n"
     ]
    }
   ],
   "source": [
    "# for the probability smoothing\n",
    "NUMERATOR_SMOOTHING = 1 # alpha in https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "DENOMINATOR_SMOOTHING = 10**4 # d in https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "\n",
    "'''This function calculates both prior and conditional probabilities in order to calculate\n",
    "   the log likelihood of a sentence by running over each word in the sentence. \n",
    "   This calculation happens for both bigram and triagram langauge models.'''\n",
    "\n",
    "def sentence_log_probabilty(unigrams : Counter, bigrams  : Counter,trigrams : Counter, sentence: str):\n",
    "    bigram_log_likelilhood, trigram_log_likelilhood = 0, 0           #initializing the log likelihood for both bigram and triagram langauge models to zero\n",
    "    words_in_sentence = sentence.split()                             #spiliting the sentence into a list of words\n",
    "    n_words = len(words_in_sentence)                                 #a variable that holds the Length of the sentence\n",
    "    for index, word in  enumerate(words_in_sentence):                #runing over each word in the list next to an index\n",
    "        '''Each prior/conditional probability is smoothed by Laplace Smoothing'''\n",
    "        if index==0:                                                 #if its the first word in the list\n",
    "            count_uni_word=unigrams[word]                            #counting how many times the first word appears in our corpus\n",
    "                                                                     #adding log of the prior propability of the first word to the bigram log likelihood\n",
    "            bigram_log_likelilhood+=np.log(((count_uni_word)+NUMERATOR_SMOOTHING)/(sum(unigrams.values())+NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "                                                                     #counting how many times the first+second combination of words appears in our corpus\n",
    "            count_bi_word=bigrams[(word,words_in_sentence[1])]       #(spider man)\n",
    "                                                                     #adding log of the prior propability of the first+second combination of words to the trigram log likelihood\n",
    "            trigram_log_likelilhood+=np.log(((count_bi_word)+NUMERATOR_SMOOTHING)/(sum(bigrams.values())+NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "                                                                     #counting how many times the first+second+third combination of words appears in our corpus\n",
    "            count_tri_word=trigrams[(word,words_in_sentence[1],words_in_sentence[2])]\n",
    "                                                                     #adding log of the first conditional probability to the bigram log likelihood (count_bi_word divided by count_uni_word)\n",
    "            bigram_log_likelilhood+=np.log((count_bi_word+NUMERATOR_SMOOTHING)/(count_uni_word+NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "                                                                     #adding log of the first conditional probability to the trigram log likelihood (count_tri_word divided by count_bi_word)\n",
    "            trigram_log_likelilhood+=np.log((count_tri_word+NUMERATOR_SMOOTHING)/(count_bi_word+NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "            continue\n",
    "        if index in range(1,n_words-1):                               #if its not the first/last word\n",
    "            count_uni_word=unigrams[word]                             #counting how many times the word appears in our corpus\n",
    "            count_bi_word=bigrams[(word,words_in_sentence[index+1])]  #counting how many times the combination of the current word and next word appears in our corpus\n",
    "                                                                      #adding log of a conditional probability to the bigram log likelihood (count_bi_word divided by count_uni_word)\n",
    "            bigram_log_likelilhood+=np.log((count_bi_word+NUMERATOR_SMOOTHING)/(count_uni_word+NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "        if index in range(1,n_words-2):                               #if its not the first/last two words\n",
    "                                                                      #counting how many times the combination of the current word and next two words appears in our corpus\n",
    "            count_tri_word=trigrams[(word,words_in_sentence[index+1],words_in_sentence[index+2])]\n",
    "                                                                      #adding log of a conditional probability to the trigram log likelihood (count_tri_word divided by count_bi_word)\n",
    "            trigram_log_likelilhood+=np.log((count_tri_word+NUMERATOR_SMOOTHING)/(count_bi_word+NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "        \n",
    "    print(F\"Bigram log likelihood is {bigram_log_likelilhood}\")\n",
    "    print(F\"Trigram log likelihood is {trigram_log_likelilhood}\")\n",
    "    \n",
    "sentence = \"spider man spider man does whatever a spider can\"\n",
    "sentence_log_probabilty(tf_idf.unigram_count, tf_idf.bigram_count, tf_idf.trigram_count, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.51 Language model: B\n",
    "For each model what is the next word prediciton for the sentnence \"i am\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next word in the biagram model: ll\n",
      "next word in the trigram model: oh\n"
     ]
    }
   ],
   "source": [
    "'''After the initial word processing operations we found out that the word 'am' is in the inverted index dictionary,\n",
    "   so we chose to return the word with the max log likelihood of the sentence \"i am word\" after running over each word \n",
    "   in the corpus, for both bigram and trigram language models, instead of choosing a random word.'''\n",
    "\n",
    "#because P(i)*P(am|i) is permanent in each calculation of the bigram log likelihood -> we will only calculate the coditional probability P(word|am)\n",
    "#because P(i am) is permanent in each calculation of the trigram log likelihood -> we will only calculate the coditional probability P(word|i am)\n",
    "\n",
    "max_bi, max_tri=-2**100,-2**100                    #initializing the max probabilty for both models to a large negative number\n",
    "for word in tf_idf.unigram_count.keys():           #running over each unique word in our corpus\n",
    "                                                   #P(word|am) with Laplace Smoothing\n",
    "    prob_bi=np.log(((tf_idf.bigram_count[('am',word)])+NUMERATOR_SMOOTHING)/(tf_idf.unigram_count['am']+NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "                                                #P(word|i am) with Laplace Smoothing\n",
    "    prob_tri=np.log(((tf_idf.trigram_count[('i','am',word)])+NUMERATOR_SMOOTHING)/(tf_idf.bigram_count[('i', 'am')]+NUMERATOR_SMOOTHING*DENOMINATOR_SMOOTHING))\n",
    "    if prob_bi>max_bi:                             #if the bigram probability is larger than the max bigram probabilty\n",
    "        max_bi=prob_bi                             #change the max probability to the current bigram probability\n",
    "        next_word_bi=word                          #save the current word as the predicted word in the bigram model\n",
    "    if prob_tri>max_tri:                           #if the trigram probability is larger than the max trigram probabilty\n",
    "        max_tri=prob_tri                           #change the max probability to the current trigram probability\n",
    "        next_word_tri=word                         #save the current word as the predicted word in the trigram model\n",
    "\n",
    "print ('next word in the biagram model:',next_word_bi)\n",
    "print ('next word in the trigram model:',next_word_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
